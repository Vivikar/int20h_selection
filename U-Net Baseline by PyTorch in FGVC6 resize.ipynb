{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This kernel is U-Net Baseline written by PyTorch\nIn this kernel, there are many places that are simplified now.  \nSo, you should fix these bad points.  \n\n[U-Net web site](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)  \n[U-Net paper](https://arxiv.org/abs/1505.04597)  \n\nI reference [this blog post](https://lp-tech.net/articles/hzfn7?page=2  ) in U-Net installation.  \nThank you awesome this blog post.  \n\nThis is [my EDA](https://www.kaggle.com/go1dfish/fgvc6-simple-eda).  \nIf you don't know this competition rule and data, this EDA might help you.  "},{"metadata":{},"cell_type":"markdown","source":"# Import modules"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch.autograd import Function, Variable\nfrom pathlib import Path\nfrom itertools import groupby","execution_count":1,"outputs":[{"output_type":"stream","text":"['train.csv', 'train', 'label_descriptions.json', 'test', 'sample_submission.csv']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = \"../input/\"\ntrain_img_dir = \"../input/train/\"\ntest_img_dir = \"../input/test/\"\n\nWIDTH = 512\nHEIGHT = 512\ncategory_num = 46 + 1\n\nratio = 8\n\nepoch_num = 8\nbatch_size = 4\n\ndevice = \"cuda:0\"","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(os.listdir(\"../input/train/\"))","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"45195"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(os.listdir(\"../input/test/\"))","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"3193"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(input_dir + \"train.csv\")\ntrain_df.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"                                ImageId   ...   ClassId\n0  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   ...         6\n1  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   ...         0\n2  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   ...        28\n3  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   ...        31\n4  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   ...        32\n\n[5 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>EncodedPixels</th>\n      <th>Height</th>\n      <th>Width</th>\n      <th>ClassId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>6068157 7 6073371 20 6078584 34 6083797 48 608...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>6323163 11 6328356 32 6333549 53 6338742 75 63...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>8521389 10 8526585 30 8531789 42 8537002 46 85...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>12903854 2 12909064 7 12914275 10 12919485 15 ...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>10837337 5 10842542 14 10847746 24 10852951 33...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>32</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(331213, 5)"},"metadata":{}}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Define utils\nFor simplicity, It focus only category"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_onehot_vec(x):\n    vec = np.zeros(category_num)\n    vec[x] = 1\n    return vec","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_mask_img(segment_df):\n    seg_width = segment_df.at[0, \"Width\"]\n    seg_height = segment_df.at[0, \"Height\"]\n    seg_img = np.full(seg_width*seg_height, category_num-1, dtype=np.int32)\n    for encoded_pixels, class_id in zip(segment_df[\"EncodedPixels\"].values, segment_df[\"ClassId\"].values):\n        pixel_list = list(map(int, encoded_pixels.split(\" \")))\n        for i in range(0, len(pixel_list), 2):\n            start_index = pixel_list[i] - 1\n            index_len = pixel_list[i+1] - 1\n            seg_img[start_index:start_index+index_len] = int(class_id.split(\"_\")[0])\n    seg_img = seg_img.reshape((seg_height, seg_width), order='F')\n    seg_img = cv2.resize(seg_img, (WIDTH, HEIGHT), interpolation=cv2.INTER_NEAREST)\n    \"\"\"\n    seg_img_onehot = np.zeros((HEIGHT, WIDTH, category_num), dtype=np.int32)\n    #seg_img_onehot = np.zeros((seg_height//ratio, seg_width//ratio, category_num), dtype=np.int32)\n    # OPTIMIZE: slow\n    for ind in range(HEIGHT):\n        for col in range(WIDTH):\n            seg_img_onehot[ind, col] = make_onehot_vec(seg_img[ind, col])\n    \"\"\"\n    return seg_img","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_generator(df, batch_size):\n    img_ind_num = df.groupby(\"ImageId\")[\"ClassId\"].count()\n    index = df.index.values[0]\n    trn_images = []\n    seg_images = []\n    for i, (img_name, ind_num) in enumerate(img_ind_num.items()):\n        img = cv2.imread(train_img_dir + img_name)\n        img = cv2.resize(img, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)\n        segment_df = (df.loc[index:index+ind_num-1, :]).reset_index(drop=True)\n        index += ind_num\n        if segment_df[\"ImageId\"].nunique() != 1:\n            raise Exception(\"Index Range Error\")\n        seg_img = make_mask_img(segment_df)\n        \n        # HWC -> CHW\n        img = img.transpose((2, 0, 1))\n        #seg_img = seg_img.transpose((2, 0, 1))\n        \n        trn_images.append(img)\n        seg_images.append(seg_img)\n        if((i+1) % batch_size == 0):\n            yield np.array(trn_images, dtype=np.float32) / 255, np.array(seg_images, dtype=np.int32)\n            trn_images = []\n            seg_images = []\n    if(len(trn_images) != 0):\n        yield np.array(trn_images, dtype=np.float32) / 255, np.array(seg_images, dtype=np.int32)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_generator(df):\n    img_names = df[\"ImageId\"].values\n    for img_name in img_names:\n        img = cv2.imread(test_img_dir + img_name)\n        img = cv2.resize(img, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)\n        # HWC -> CHW\n        img = img.transpose((2, 0, 1))\n        yield img_name, np.asarray([img], dtype=np.float32) / 255","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(input_string):\n    return [(len(list(g)), k) for k,g in groupby(input_string)]\n\ndef run_length(label_vec):\n    encode_list = encode(label_vec)\n    index = 1\n    class_dict = {}\n    for i in encode_list:\n        if i[1] != category_num-1:\n            if i[1] not in class_dict.keys():\n                class_dict[i[1]] = []\n            class_dict[i[1]] = class_dict[i[1]] + [index, i[0]]\n        index += i[0]\n    return class_dict","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"class double_conv(nn.Module):\n    '''(conv => BN => ReLU) * 2'''\n    def __init__(self, in_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass inconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(inconv, self).__init__()\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(down, self).__init__()\n        self.mpconv = nn.Sequential(\n            nn.MaxPool2d(2),\n            double_conv(in_ch, out_ch)\n        )\n\n    def forward(self, x):\n        x = self.mpconv(x)\n        return x\n\n\nclass up(nn.Module):\n    def __init__(self, in_ch, out_ch, bilinear=True):\n        super(up, self).__init__()\n\n        #  would be a nice idea if the upsampling could be learned too,\n        #  but my machine do not have enough memory to handle all those weights\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        else:\n            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        diffX = x1.size()[2] - x2.size()[2]\n        diffY = x1.size()[3] - x2.size()[3]\n        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n                        diffY // 2, int(diffY / 2)))\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x\n\n\nclass outconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(outconv, self).__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n    \nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes):\n        super(UNet, self).__init__()\n        self.inc = inconv(n_channels, 64)\n        self.down1 = down(64, 128)\n        self.down2 = down(128, 256)\n        self.down3 = down(256, 512)\n        self.down4 = down(512, 512)\n        self.up1 = up(1024, 256)\n        self.up2 = up(512, 128)\n        self.up3 = up(256, 64)\n        self.up4 = up(128, 64)\n        self.outc = outconv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return x","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"(331213, 5)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"333415 // 4","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"83353"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.iloc[83348:83354, :]","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"                                    ImageId   ...   ClassId\n83348  404b0d1b58a1af0841e1e46f975118eb.jpg   ...        22\n83349  404b0d1b58a1af0841e1e46f975118eb.jpg   ...         8\n83350  404b0d1b58a1af0841e1e46f975118eb.jpg   ...         0\n83351  404b0d1b58a1af0841e1e46f975118eb.jpg   ...        28\n83352  404b0d1b58a1af0841e1e46f975118eb.jpg   ...        31\n83353  404b0d1b58a1af0841e1e46f975118eb.jpg   ...        31\n\n[6 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>EncodedPixels</th>\n      <th>Height</th>\n      <th>Width</th>\n      <th>ClassId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>83348</th>\n      <td>404b0d1b58a1af0841e1e46f975118eb.jpg</td>\n      <td>337743 2 338739 6 339735 10 340732 13 341728 1...</td>\n      <td>1000</td>\n      <td>667</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>83349</th>\n      <td>404b0d1b58a1af0841e1e46f975118eb.jpg</td>\n      <td>235561 2 236558 5 237554 9 238551 13 239549 15...</td>\n      <td>1000</td>\n      <td>667</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>83350</th>\n      <td>404b0d1b58a1af0841e1e46f975118eb.jpg</td>\n      <td>219425 10 220411 30 221398 50 222385 68 223374...</td>\n      <td>1000</td>\n      <td>667</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>83351</th>\n      <td>404b0d1b58a1af0841e1e46f975118eb.jpg</td>\n      <td>298226 2 299225 7 300225 11 301225 14 302224 1...</td>\n      <td>1000</td>\n      <td>667</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>83352</th>\n      <td>404b0d1b58a1af0841e1e46f975118eb.jpg</td>\n      <td>379364 12 380342 1 380352 27 381340 4 381351 3...</td>\n      <td>1000</td>\n      <td>667</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>83353</th>\n      <td>404b0d1b58a1af0841e1e46f975118eb.jpg</td>\n      <td>219425 10 220411 30 221398 50 222385 68 223374...</td>\n      <td>1000</td>\n      <td>667</td>\n      <td>31</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.iloc[73350:73354, :]","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"                                    ImageId   ...   ClassId\n73350  3892703cf1e7f6a3cc1892fcb3daedae.jpg   ...        31\n73351  3892703cf1e7f6a3cc1892fcb3daedae.jpg   ...         1\n73352  3893868714043964534a6cdbaf1f0d75.jpg   ...        23\n73353  3893868714043964534a6cdbaf1f0d75.jpg   ...        23\n\n[4 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>EncodedPixels</th>\n      <th>Height</th>\n      <th>Width</th>\n      <th>ClassId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>73350</th>\n      <td>3892703cf1e7f6a3cc1892fcb3daedae.jpg</td>\n      <td>85434 2 88642 6 91851 9 95059 14 98268 17 1014...</td>\n      <td>3211</td>\n      <td>2141</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>73351</th>\n      <td>3892703cf1e7f6a3cc1892fcb3daedae.jpg</td>\n      <td>82220 2 85428 6 88637 9 91846 13 95055 16 9826...</td>\n      <td>3211</td>\n      <td>2141</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>73352</th>\n      <td>3893868714043964534a6cdbaf1f0d75.jpg</td>\n      <td>7536415 56 7542028 60 7547642 62 7553255 65 75...</td>\n      <td>5616</td>\n      <td>3744</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>73353</th>\n      <td>3893868714043964534a6cdbaf1f0d75.jpg</td>\n      <td>12534531 11 12540144 33 12545757 55 12551369 6...</td>\n      <td>5616</td>\n      <td>3744</td>\n      <td>23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"For simplicity, use about 25% data.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"net = UNet(n_channels=3, n_classes=category_num).to(device)\n\noptimizer = optim.SGD(\n    net.parameters(),\n    lr=0.1,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\ncriterion = nn.CrossEntropyLoss()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_sta = 73352\nval_end = 83351\ntrain_loss = []\nvalid_loss = []\nfor epoch in range(epoch_num):\n    epoch_trn_loss = 0\n    train_len = 0\n    net.train()\n    for iteration, (X_trn, Y_trn) in enumerate(tqdm(train_generator(train_df.iloc[:val_sta, :], batch_size))):\n        X = torch.tensor(X_trn, dtype=torch.float32).to(device)\n        Y = torch.tensor(Y_trn, dtype=torch.long).to(device)\n        train_len += len(X)\n        \n        #Y_flat = Y.view(-1)\n        mask_pred = net(X)\n        #mask_prob = torch.softmax(mask_pred, dim=1)\n        #mask_prob_flat = mask_prob.view(-1)\n        loss = criterion(mask_pred, Y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_trn_loss += loss.item()\n        \n        if iteration % 100 == 0:\n            print(\"train loss in {:0>2}epoch  /{:>5}iter:    {:<10.8}\".format(epoch+1, iteration, epoch_trn_loss/(iteration+1)))\n        \n    train_loss.append(epoch_trn_loss/(iteration+1))\n    print(\"train {}epoch loss({}iteration):    {:10.8}\".format(epoch+1, iteration, train_loss[-1]))\n    \n    epoch_val_loss = 0\n    val_len = 0\n    net.eval()\n    for iteration, (X_val, Y_val) in enumerate(tqdm(train_generator(train_df.iloc[val_sta:val_end, :], batch_size))):\n        X = torch.tensor(X_val, dtype=torch.float32).to(device)\n        Y = torch.tensor(Y_val, dtype=torch.long).to(device)\n        val_len += len(X)\n        \n        #Y_flat = Y.view(-1)\n        \n        mask_pred = net(X)\n        #mask_prob = torch.softmax(mask_pred, dim=1)\n        #mask_prob_flat = mask_prob.view(-1)\n        loss = criterion(mask_pred, Y)\n        epoch_val_loss += loss.item()\n        \n        if iteration % 100 == 0:\n            print(\"valid loss in {:0>2}epoch  /{:>5}iter:    {:<10.8}\".format(epoch+1, iteration, epoch_val_loss/(iteration+1)))\n        \n    valid_loss.append(epoch_val_loss/(iteration+1))\n    print(\"valid {}epoch loss({}iteration):    {:10.8}\".format(epoch+1, iteration, valid_loss[-1]))","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1972cb73c908421eb0a889d956c5b0c9"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n","name":"stderr"},{"output_type":"stream","text":"train loss in 01epoch  /    0iter:    3.7808681 \ntrain loss in 01epoch  /  100iter:    1.2731448 \ntrain loss in 01epoch  /  200iter:    1.1407036 \ntrain loss in 01epoch  /  300iter:    1.1156803 \ntrain loss in 01epoch  /  400iter:    1.0912886 \ntrain loss in 01epoch  /  500iter:    1.0671572 \ntrain loss in 01epoch  /  600iter:    1.0548593 \ntrain loss in 01epoch  /  700iter:    1.0494163 \ntrain loss in 01epoch  /  800iter:    1.0420111 \ntrain loss in 01epoch  /  900iter:    1.0390621 \ntrain loss in 01epoch  / 1000iter:    1.0276598 \ntrain loss in 01epoch  / 1100iter:    1.0224457 \ntrain loss in 01epoch  / 1200iter:    1.0185198 \ntrain loss in 01epoch  / 1300iter:    1.0184505 \ntrain loss in 01epoch  / 1400iter:    1.0162778 \ntrain loss in 01epoch  / 1500iter:    1.0127839 \ntrain loss in 01epoch  / 1600iter:    1.0096741 \ntrain loss in 01epoch  / 1700iter:    1.0068054 \ntrain loss in 01epoch  / 1800iter:    1.0027825 \ntrain loss in 01epoch  / 1900iter:    1.0022021 \ntrain loss in 01epoch  / 2000iter:    1.0010571 \ntrain loss in 01epoch  / 2100iter:    0.99729397\ntrain loss in 01epoch  / 2200iter:    0.99465385\ntrain loss in 01epoch  / 2300iter:    0.99280728\ntrain loss in 01epoch  / 2400iter:    0.98973118\ntrain loss in 01epoch  / 2500iter:    0.98775962\n\ntrain 1epoch loss(2503iteration):     0.9875957\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bf8d79051ea4d40965fc979801dddf0"}},"metadata":{}},{"output_type":"stream","text":"valid loss in 01epoch  /    0iter:    0.99603325\nvalid loss in 01epoch  /  100iter:    0.95233545\nvalid loss in 01epoch  /  200iter:    0.95109498\nvalid loss in 01epoch  /  300iter:    0.94471123\n\nvalid 1epoch loss(347iteration):    0.93913579\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8061a1a4d3714c49a64d0247c6f0c178"}},"metadata":{}},{"output_type":"stream","text":"train loss in 02epoch  /    0iter:    1.1434746 \ntrain loss in 02epoch  /  100iter:    0.95416942\ntrain loss in 02epoch  /  200iter:    0.91928346\ntrain loss in 02epoch  /  300iter:    0.92926362\ntrain loss in 02epoch  /  400iter:    0.92423466\ntrain loss in 02epoch  /  500iter:    0.91528389\ntrain loss in 02epoch  /  600iter:    0.91618777\ntrain loss in 02epoch  /  700iter:    0.92091957\ntrain loss in 02epoch  /  800iter:    0.92140739\ntrain loss in 02epoch  /  900iter:    0.92528268\ntrain loss in 02epoch  / 1000iter:    0.91893268\ntrain loss in 02epoch  / 1100iter:    0.91874149\ntrain loss in 02epoch  / 1200iter:    0.91952493\ntrain loss in 02epoch  / 1300iter:    0.92368216\ntrain loss in 02epoch  / 1400iter:    0.92462523\ntrain loss in 02epoch  / 1500iter:    0.9236668 \ntrain loss in 02epoch  / 1600iter:    0.9230057 \ntrain loss in 02epoch  / 1700iter:    0.92280991\ntrain loss in 02epoch  / 1800iter:    0.92132775\ntrain loss in 02epoch  / 1900iter:    0.922619  \ntrain loss in 02epoch  / 2000iter:    0.92382814\ntrain loss in 02epoch  / 2100iter:    0.92291567\ntrain loss in 02epoch  / 2200iter:    0.92232363\ntrain loss in 02epoch  / 2300iter:    0.92264711\ntrain loss in 02epoch  / 2400iter:    0.92090098\ntrain loss in 02epoch  / 2500iter:    0.92035755\n\ntrain 2epoch loss(2503iteration):    0.92024707\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e841c198d0b84146a856f8d7b025ea7a"}},"metadata":{}},{"output_type":"stream","text":"valid loss in 02epoch  /    0iter:    1.0323303 \nvalid loss in 02epoch  /  100iter:    0.97016995\nvalid loss in 02epoch  /  200iter:    0.96665149\nvalid loss in 02epoch  /  300iter:    0.96176101\n\nvalid 2epoch loss(347iteration):    0.95533126\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9134a39a10814f0ca1a0bbf5253e87d6"}},"metadata":{}},{"output_type":"stream","text":"train loss in 03epoch  /    0iter:    1.119164  \ntrain loss in 03epoch  /  100iter:    0.89978937\ntrain loss in 03epoch  /  200iter:    0.86855576\ntrain loss in 03epoch  /  300iter:    0.88311315\ntrain loss in 03epoch  /  400iter:    0.87969414\ntrain loss in 03epoch  /  500iter:    0.8743736 \ntrain loss in 03epoch  /  600iter:    0.87471522\ntrain loss in 03epoch  /  700iter:    0.88058176\ntrain loss in 03epoch  /  800iter:    0.88194678\ntrain loss in 03epoch  /  900iter:    0.88453062\ntrain loss in 03epoch  / 1000iter:    0.87844556\ntrain loss in 03epoch  / 1100iter:    0.87791805\ntrain loss in 03epoch  / 1200iter:    0.87733794\ntrain loss in 03epoch  / 1300iter:    0.88029306\ntrain loss in 03epoch  / 1400iter:    0.88141336\ntrain loss in 03epoch  / 1500iter:    0.88006093\ntrain loss in 03epoch  / 1600iter:    0.87915871\ntrain loss in 03epoch  / 1700iter:    0.87786528\ntrain loss in 03epoch  / 1800iter:    0.87575238\ntrain loss in 03epoch  / 1900iter:    0.87650714\ntrain loss in 03epoch  / 2000iter:    0.87654885\ntrain loss in 03epoch  / 2100iter:    0.87422378\ntrain loss in 03epoch  / 2200iter:    0.87335501\ntrain loss in 03epoch  / 2300iter:    0.87304961\ntrain loss in 03epoch  / 2400iter:    0.87050522\ntrain loss in 03epoch  / 2500iter:    0.86943921\n\ntrain 3epoch loss(2503iteration):    0.86927469\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8938196b5c534472a4c2964300d0abca"}},"metadata":{}},{"output_type":"stream","text":"valid loss in 03epoch  /    0iter:    0.95003051\nvalid loss in 03epoch  /  100iter:    0.86771248\nvalid loss in 03epoch  /  200iter:    0.8689741 \nvalid loss in 03epoch  /  300iter:    0.86176526\n\nvalid 3epoch loss(347iteration):    0.85679855\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bea2893d25e44078d26db27a5eae77f"}},"metadata":{}},{"output_type":"stream","text":"train loss in 04epoch  /    0iter:    0.98412013\ntrain loss in 04epoch  /  100iter:    0.83188903\ntrain loss in 04epoch  /  200iter:    0.80185034\ntrain loss in 04epoch  /  300iter:    0.81798989\ntrain loss in 04epoch  /  400iter:    0.81439289\ntrain loss in 04epoch  /  500iter:    0.80786247\ntrain loss in 04epoch  /  600iter:    0.80883221\ntrain loss in 04epoch  /  700iter:    0.8137571 \ntrain loss in 04epoch  /  800iter:    0.81459833\ntrain loss in 04epoch  /  900iter:    0.81832467\ntrain loss in 04epoch  / 1000iter:    0.81271733\ntrain loss in 04epoch  / 1100iter:    0.81236861\ntrain loss in 04epoch  / 1200iter:    0.81408431\ntrain loss in 04epoch  / 1300iter:    0.8184432 \ntrain loss in 04epoch  / 1400iter:    0.82054794\ntrain loss in 04epoch  / 1500iter:    0.81997347\ntrain loss in 04epoch  / 1600iter:    0.81964039\ntrain loss in 04epoch  / 1700iter:    0.81930624\ntrain loss in 04epoch  / 1800iter:    0.81738777\ntrain loss in 04epoch  / 1900iter:    0.81894584\ntrain loss in 04epoch  / 2000iter:    0.81951199\ntrain loss in 04epoch  / 2100iter:    0.8178945 \ntrain loss in 04epoch  / 2200iter:    0.81730495\ntrain loss in 04epoch  / 2300iter:    0.81730638\ntrain loss in 04epoch  / 2400iter:    0.81584189\ntrain loss in 04epoch  / 2500iter:    0.81503957\n\ntrain 4epoch loss(2503iteration):    0.81491136\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"075600e59a16474b8b191de6a0bfbe6a"}},"metadata":{}},{"output_type":"stream","text":"valid loss in 04epoch  /    0iter:    0.82034671\nvalid loss in 04epoch  /  100iter:    0.80622371\nvalid loss in 04epoch  /  200iter:    0.80747206\nvalid loss in 04epoch  /  300iter:    0.79795844\n\nvalid 4epoch loss(347iteration):    0.79404497\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae574ab5921342748e141409bbc41eba"}},"metadata":{}},{"output_type":"stream","text":"train loss in 05epoch  /    0iter:    0.92434621\ntrain loss in 05epoch  /  100iter:    0.7897866 \ntrain loss in 05epoch  /  200iter:    0.76176047\ntrain loss in 05epoch  /  300iter:    0.77944031\ntrain loss in 05epoch  /  400iter:    0.77564479\ntrain loss in 05epoch  /  500iter:    0.76993612\ntrain loss in 05epoch  /  600iter:    0.7711833 \ntrain loss in 05epoch  /  700iter:    0.77625266\ntrain loss in 05epoch  /  800iter:    0.77770374\ntrain loss in 05epoch  /  900iter:    0.78233663\ntrain loss in 05epoch  / 1000iter:    0.77675858\ntrain loss in 05epoch  / 1100iter:    0.77733959\ntrain loss in 05epoch  / 1200iter:    0.77910445\ntrain loss in 05epoch  / 1300iter:    0.78300459\ntrain loss in 05epoch  / 1400iter:    0.78435741\ntrain loss in 05epoch  / 1500iter:    0.78338519\ntrain loss in 05epoch  / 1600iter:    0.78345333\ntrain loss in 05epoch  / 1700iter:    0.7833541 \ntrain loss in 05epoch  / 1800iter:    0.78154359\ntrain loss in 05epoch  / 1900iter:    0.78368111\ntrain loss in 05epoch  / 2000iter:    0.78415364\ntrain loss in 05epoch  / 2100iter:    0.78231549\ntrain loss in 05epoch  / 2200iter:    0.78205624\ntrain loss in 05epoch  / 2300iter:    0.78243944\ntrain loss in 05epoch  / 2400iter:    0.78116808\ntrain loss in 05epoch  / 2500iter:    0.78116111\n\ntrain 5epoch loss(2503iteration):    0.78105157\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7139707659c343f9940e423d4c5e4990"}},"metadata":{}},{"output_type":"stream","text":"valid loss in 05epoch  /    0iter:    0.87086761\nvalid loss in 05epoch  /  100iter:    0.82476387\nvalid loss in 05epoch  /  200iter:    0.82691018\nvalid loss in 05epoch  /  300iter:    0.81737564\n\nvalid 5epoch loss(347iteration):    0.81160405\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9681477e11e4102be4d74aec18290e6"}},"metadata":{}},{"output_type":"stream","text":"train loss in 06epoch  /    0iter:    0.86080074\ntrain loss in 06epoch  /  100iter:    0.77937379\ntrain loss in 06epoch  /  200iter:    0.74536101\ntrain loss in 06epoch  /  300iter:    0.75874652\ntrain loss in 06epoch  /  400iter:    0.75319051\ntrain loss in 06epoch  /  500iter:    0.74528783\ntrain loss in 06epoch  /  600iter:    0.74776724\ntrain loss in 06epoch  /  700iter:    0.7525514 \ntrain loss in 06epoch  /  800iter:    0.75526603\ntrain loss in 06epoch  /  900iter:    0.76428981\ntrain loss in 06epoch  / 1000iter:    0.76089583\ntrain loss in 06epoch  / 1100iter:    0.76163677\ntrain loss in 06epoch  / 1200iter:    0.76316524\ntrain loss in 06epoch  / 1300iter:    0.76714819\ntrain loss in 06epoch  / 1400iter:    0.76970202\ntrain loss in 06epoch  / 1500iter:    0.76896942\ntrain loss in 06epoch  / 1600iter:    0.76877468\ntrain loss in 06epoch  / 1700iter:    0.76876114\ntrain loss in 06epoch  / 1800iter:    0.76755997\ntrain loss in 06epoch  / 1900iter:    0.76980683\ntrain loss in 06epoch  / 2000iter:    0.77077644\ntrain loss in 06epoch  / 2100iter:    0.76988208\ntrain loss in 06epoch  / 2200iter:    0.76977211\ntrain loss in 06epoch  / 2300iter:    0.7702444 \ntrain loss in 06epoch  / 2400iter:    0.76855759\ntrain loss in 06epoch  / 2500iter:    0.76800258\n\ntrain 6epoch loss(2503iteration):    0.76788881\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d04fe3ebc143cfbd4fea50bc300821"}},"metadata":{}},{"output_type":"stream","text":"valid loss in 06epoch  /    0iter:    0.75528651\nvalid loss in 06epoch  /  100iter:    0.80938272\nvalid loss in 06epoch  /  200iter:    0.81343751\nvalid loss in 06epoch  /  300iter:    0.80768086\n\nvalid 6epoch loss(347iteration):    0.80217946\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e0c3f7ff0f742afabc7421f5e039dbd"}},"metadata":{}},{"output_type":"stream","text":"train loss in 07epoch  /    0iter:    0.81937146\ntrain loss in 07epoch  /  100iter:    0.76041588\ntrain loss in 07epoch  /  200iter:    0.73644228\ntrain loss in 07epoch  /  300iter:    0.7529125 \ntrain loss in 07epoch  /  400iter:    0.75047421\ntrain loss in 07epoch  /  500iter:    0.74310587\ntrain loss in 07epoch  /  600iter:    0.74382038\ntrain loss in 07epoch  /  700iter:    0.74776025\ntrain loss in 07epoch  /  800iter:    0.74911626\ntrain loss in 07epoch  /  900iter:    0.7526035 \ntrain loss in 07epoch  / 1000iter:    0.74741268\ntrain loss in 07epoch  / 1100iter:    0.74697296\ntrain loss in 07epoch  / 1200iter:    0.74718113\ntrain loss in 07epoch  / 1300iter:    0.75096219\ntrain loss in 07epoch  / 1400iter:    0.75250301\ntrain loss in 07epoch  / 1500iter:    0.75143292\ntrain loss in 07epoch  / 1600iter:    0.7517867 \ntrain loss in 07epoch  / 1700iter:    0.7523281 \ntrain loss in 07epoch  / 1800iter:    0.75091743\ntrain loss in 07epoch  / 1900iter:    0.7528693 \ntrain loss in 07epoch  / 2000iter:    0.75371404\ntrain loss in 07epoch  / 2100iter:    0.75234498\ntrain loss in 07epoch  / 2200iter:    0.75274864\ntrain loss in 07epoch  / 2300iter:    0.75351968\ntrain loss in 07epoch  / 2400iter:    0.75188234\ntrain loss in 07epoch  / 2500iter:    0.75136924\n\ntrain 7epoch loss(2503iteration):    0.75127114\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c7ec11bd044d9d84b328f862d05da3"}},"metadata":{}},{"output_type":"stream","text":"valid loss in 07epoch  /    0iter:    0.83411443\nvalid loss in 07epoch  /  100iter:    0.81385857\nvalid loss in 07epoch  /  200iter:    0.81966688\nvalid loss in 07epoch  /  300iter:    0.81518302\n\nvalid 7epoch loss(347iteration):    0.80929136\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95d3096f2c3947dfaba78ed6ce2bd572"}},"metadata":{}},{"output_type":"stream","text":"train loss in 08epoch  /    0iter:    0.84060639\ntrain loss in 08epoch  /  100iter:    0.74190513\ntrain loss in 08epoch  /  200iter:    0.7182923 \ntrain loss in 08epoch  /  300iter:    0.73472241\ntrain loss in 08epoch  /  400iter:    0.73045697\ntrain loss in 08epoch  /  500iter:    0.72378685\ntrain loss in 08epoch  /  600iter:    0.72711251\ntrain loss in 08epoch  /  700iter:    0.73225973\ntrain loss in 08epoch  /  800iter:    0.73342164\ntrain loss in 08epoch  /  900iter:    0.73868762\ntrain loss in 08epoch  / 1000iter:    0.73437165\ntrain loss in 08epoch  / 1100iter:    0.73351673\ntrain loss in 08epoch  / 1200iter:    0.73457308\ntrain loss in 08epoch  / 1300iter:    0.73843175\ntrain loss in 08epoch  / 1400iter:    0.74094191\ntrain loss in 08epoch  / 1500iter:    0.74047159\ntrain loss in 08epoch  / 1600iter:    0.74063366\ntrain loss in 08epoch  / 1700iter:    0.74148685\ntrain loss in 08epoch  / 1800iter:    0.74039576\ntrain loss in 08epoch  / 1900iter:    0.74231388\ntrain loss in 08epoch  / 2000iter:    0.74332962\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-7787679ee145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_sta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-1f262b255053>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(df, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msegment_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ImageId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Index Range Error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mseg_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_mask_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# HWC -> CHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-7c529076807d>\u001b[0m in \u001b[0;36mmake_mask_img\u001b[0;34m(segment_df)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mseg_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_width\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_num\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mencoded_pixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EncodedPixels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ClassId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpixel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_pixels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.plot(list(range(epoch_num)), train_loss, color='green')\n#plt.plot(list(range(epoch_num)), valid_loss, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":19,"outputs":[{"output_type":"stream","text":"/kaggle/working\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(net.state_dict(), 'Unet_model')","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = pd.read_csv(input_dir + \"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import torch\nimport gc\nfor obj in gc.get_objects():\n    try:\n        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n            print(type(obj), obj.size())\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_list = []\nnet.eval()\nfor img_name, img in test_generator(sample_df):\n    X = torch.tensor(img, dtype=torch.float32).to(device)\n    mask_pred = net(X)\n    mask_pred = mask_pred.cpu().detach().numpy()\n    mask_prob = np.argmax(mask_pred, axis=1)\n    mask_prob = mask_prob.ravel(order='F')\n    class_dict = run_length(mask_prob)\n    if len(class_dict) == 0:\n        sub_list.append([img_name, \"1 1\", 1])\n    else:\n        for key, val in class_dict.items():\n            sub_list.append([img_name, \" \".join(map(str, val)), key])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame(sub_list, columns=sample_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thank you for watching!\nPlease tell me when I make mistakes in program and English.  \nI hope this kernel will help.  \nIf you think this kernel is useful, please upvote. If you do, I feel happy and get enough sleep.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}